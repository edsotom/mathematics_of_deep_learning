{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3cb549a",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Models-Discussion\" data-toc-modified-id=\"Models-Discussion-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Models Discussion</a></span><ul class=\"toc-item\"><li><span><a href=\"#多Why-we-don't-implement-linear-regression?\" data-toc-modified-id=\"多Why-we-don't-implement-linear-regression?-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>多Why we don't implement linear regression?</a></span></li><li><span><a href=\"#Performance-Metrics\" data-toc-modified-id=\"Performance-Metrics-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Performance Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-metric-for-banknote-authentication-data-set\" data-toc-modified-id=\"Performance-metric-for-banknote-authentication-data-set-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Performance metric for banknote authentication data set</a></span></li><li><span><a href=\"#Performance-metric-for-occupancy-detection-data-set\" data-toc-modified-id=\"Performance-metric-for-occupancy-detection-data-set-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Performance metric for occupancy detection data set</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Processing\" data-toc-modified-id=\"Data-Processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Banknote-authentication-data-set-processing\" data-toc-modified-id=\"Banknote-authentication-data-set-processing-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Banknote authentication data set processing</a></span></li><li><span><a href=\"#Occupancy-detection-data-set-procesing\" data-toc-modified-id=\"Occupancy-detection-data-set-procesing-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Occupancy detection data set procesing</a></span></li></ul></li><li><span><a href=\"#Models-Implementation\" data-toc-modified-id=\"Models-Implementation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Models Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-metrics-by-confusion-matrix-example\" data-toc-modified-id=\"Performance-metrics-by-confusion-matrix-example-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Performance metrics by confusion matrix example</a></span></li><li><span><a href=\"#Performance-metrics-in-banknote-authentication\" data-toc-modified-id=\"Performance-metrics-in-banknote-authentication-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Performance metrics in banknote authentication</a></span></li><li><span><a href=\"#Performance-metrics-in-occupancy-problem\" data-toc-modified-id=\"Performance-metrics-in-occupancy-problem-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Performance metrics in occupancy problem</a></span></li></ul></li><li><span><a href=\"#Decision-tree-learning\" data-toc-modified-id=\"Decision-tree-learning-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Decision tree learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-metrics-in-banknote-authentication\" data-toc-modified-id=\"Performance-metrics-in-banknote-authentication-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Performance metrics in banknote authentication</a></span></li><li><span><a href=\"#Performance-metrics-in-occupancy-detection\" data-toc-modified-id=\"Performance-metrics-in-occupancy-detection-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Performance metrics in occupancy detection</a></span></li></ul></li><li><span><a href=\"#Support-vector-machine-(SVM)\" data-toc-modified-id=\"Support-vector-machine-(SVM)-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Support vector machine (SVM)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search-for-SVM\" data-toc-modified-id=\"Grid-Search-for-SVM-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Grid Search for SVM</a></span></li><li><span><a href=\"#Performance-metrics-in-banknote-authentication\" data-toc-modified-id=\"Performance-metrics-in-banknote-authentication-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Performance metrics in banknote authentication</a></span></li><li><span><a href=\"#Performance-metrics-in-occupancy-detection\" data-toc-modified-id=\"Performance-metrics-in-occupancy-detection-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Performance metrics in occupancy detection</a></span></li></ul></li><li><span><a href=\"#K---Nearest-neighbors-(KNN)\" data-toc-modified-id=\"K---Nearest-neighbors-(KNN)-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>K - Nearest neighbors (KNN)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Performance-metrics-in-banknote-authentication\" data-toc-modified-id=\"Performance-metrics-in-banknote-authentication-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Performance metrics in banknote authentication</a></span></li><li><span><a href=\"#Performance-metrics-in-occupancy-detection\" data-toc-modified-id=\"Performance-metrics-in-occupancy-detection-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Performance metrics in occupancy detection</a></span></li></ul></li></ul></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Results</a></span></li><li><span><a href=\"#Conclusions,-Discussions-and-Future-Work\" data-toc-modified-id=\"Conclusions,-Discussions-and-Future-Work-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusions, Discussions and Future Work</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426564cb",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this project is to explore and learn about machine learning algorithms. The main purpose is to implement a few algorithms (probably the most important ML algorithms) and study their performance in two fixed datasets.\n",
    "\n",
    "Remember; for this project we are usen the following datasets: <br>\n",
    "https://archive.ics.uci.edu/ml/datasets/banknote+authentication <br>\n",
    "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
    "\n",
    "For a long discussion of these datasets please refer to First_HW jupyter notebook in the main page of the repository. \n",
    "\n",
    "# Models Discussion\n",
    "\n",
    "For each dataset we're going to implement the following models:\n",
    "<ol type=\"1\">\n",
    "   <li>Logistic Regression</li>\n",
    "   <li>Decision Tree Learning</li> \n",
    "   <li>Soft Margin Support Vector Machine (Soft-SVM)</li>\n",
    "   <li>K-Nearest Neighboors (KNN)</li>\n",
    "</ol>\n",
    "\n",
    "## 多Why we don't implement linear regression?\n",
    "\n",
    "The propose of our learning problem is predict the target outputs for new data inputs. In our problem we have binary (categorical) labels and we dont want adjust our model to the data, instead, we want to split the representation space to distinguish predicted labels of new data points. The labor of a linear regression is to adjust data, no split it. This method would be valuable if blank data appears: we would be able to fill these blank spaces with a linear regression outputs.\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "Depending of the research or bussiness objective we need to stablish a performance metric that allow us, without any type of bias, compare and decide when a model have a good or bad behavior (in the sense of generalize new data). For this reason, we set the performance metric before we implement any type of model.  This task demands thinking in the problem, and define some losses/gain criteria. <br>\n",
    "\n",
    "In order to reach a desired metric for each of our two problems, we're gonna analyce them thinking two critical events: false negative and false positive cases. For this two we're gonna decide which have more negative/possite impact in our objective depending on the problem. Remember the false negative, false positive term comes form the confusion matrix concept; the following is how sklearn implement the confusion matrix:\n",
    "\n",
    "| Actual \\ Predicted |  0 |  1 |\n",
    "|--------------------|:--:|:--:|\n",
    "|                  0 | TN | FP |\n",
    "|                  1 | FN | TP |\n",
    "\n",
    "Even if we define a performance metric for our problem, for purporses of this study, we're gonna also calculate the **accuracy score (AS)** as a standar model performance metric, remember AS is defined as:\n",
    "$$ Accuracy = \\frac{TN + TP}{TN + FN + FP + TP}$$\n",
    "\n",
    "### Performance metric for banknote authentication data set\n",
    "\n",
    "In this problem we have the task of detect whenever a banknote is genuine or not. Thinking as a seller; we note the case when a no genuine banknote entry in our cash flow (false positive case) is more important than when a genuine banknote is rejected from our cash flow (false negative case). The first creates negative movement in our cash flow, whereas, second doesn't even create a movement possite or negative movement. For this reasons we can think the false positive case must have bigger weight than a false positive. For this reasons we can conclude that the **precision score (PS)** is a good measure for assess the performance of our models: <br>\n",
    "$$\\text{Precision} = \\frac{TP}{FP + TP}$$\n",
    "\n",
    "Here, we want precision close to one, this means $FP$ must be close to zero, exactly what we want in our problem. <br>\n",
    "In the data processing phase we'll see that the classes are balanced, and no imbalanced metrics need to be implemented for this problem.\n",
    "\n",
    "### Performance metric for occupancy detection data set\n",
    "\n",
    "For this problem, our task is detect if there's someone inside a room or not. This problem can be raised from a security company, they can be interested in detect room occupancy in order to alarm when there's someone in a restricted hour, for example. In this order of ideas, if my security system is non-capable of tell me when there's a person big troubles can araise (this is a false negative case). In the other hand, if there's no one inside but my system alerts of occupancy, we can just turn off alarms and everything will be ok (this is a false positive case). For this reason we can chose **recall score (RS)** as our performance metric for this problem: <br>\n",
    "$$\\text{Recall} = \\frac{TP}{FN + TP} $$\n",
    "\n",
    "Again, we want recall close to one, this means $FN$ must be close to zero. <br>\n",
    "In the data processing phase we'll see that the classes have mild imbalanced, but we're gonna keep forward with the chosen performance metric to see if we can generalize. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697948f0",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc674f6",
   "metadata": {},
   "source": [
    "For this part we need to analyze the data sets in a statistical way and take some decisions in order to transform the data if is required (feature engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e09e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the default data manipulation library in python: pandas\n",
    "import pandas as pd \n",
    "# Numpy as the standard computation tools over arrays and matrices \n",
    "import numpy as np\n",
    "\n",
    "bank_df = pd.read_excel(\"data/banknote_data/data_banknote_authentication.xlsx\")\n",
    "\n",
    "occupancy_df = pd.read_excel(\"data/occupancy_data/occupancy_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed23607",
   "metadata": {},
   "source": [
    "## Banknote authentication data set processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8cd766e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6216</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5459</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  output\n",
       "0    3.6216    8.6661   -2.8073 -0.44699       0\n",
       "1    4.5459    8.1674   -2.4586 -1.46210       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fast first idea of the content of the data set\n",
    "bank_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85accd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we give us an idea of the size of the dataset\n",
    "bank_df.shape\n",
    "# 1372 examples, 4 features and 1 output column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26841be",
   "metadata": {},
   "source": [
    "This tell us the data has 1372 examples, 4 feature columns and 1 label column. This is what we expect from our summary of the data. <br>\n",
    "\n",
    "Remember, the labels in our data set are confusing: 0 for genuine banknotes and 1 for no genuine banknotes. This confuse our analysis of the performance metric about false positive and negative cases. For this reason, we need to swap the labels of our samples (1 label is swaped to 0 label, and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6752d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swaping the 0 labels for 1 label, and vice versa\n",
    "bank_df[\"output\"] = np.where(bank_df[\"output\"] == 1, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6999367",
   "metadata": {},
   "source": [
    "In order to comprobe if our performance metric is appropriate now we'll see if the data set is imbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "176d3cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classes</th>\n",
       "      <th>count</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>610</td>\n",
       "      <td>44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>762</td>\n",
       "      <td>56%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classes  count proportion\n",
       "0        0    610        44%\n",
       "1        1    762        56%"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the df for each label and count the number of samples\n",
    "temp_series = bank_df.groupby('output')['output'].count()\n",
    "temp_df = pd.DataFrame({'classes':temp_series.index, 'count':temp_series.values})\n",
    "temp_df['proportion'] = (temp_df['count']/bank_df.shape[0]).apply('{:.0%}'.format)\n",
    "temp_df\n",
    "# The result df give us the respective proportion of each label over the total of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495f49a",
   "metadata": {},
   "source": [
    "We can see the proportion of each class in our data set is good. Moreover, for each sample labeled as 0, we have 1.2 samples labeled as 1. This is a balanced proportion of classes. Also, this means our data set has more observations of genuine banknotes than not genuine banknotes.<br>\n",
    "\n",
    "Because the nature ot the data (each feature comes from a wavelet decomposition) we'll see the features has good behavior. <br>\n",
    "\n",
    "Now, before we split the data in train-test form, we shuffle the data set to mostly guarantee an independent sample view to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bfedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataframe\n",
    "bank_df = bank_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99a3a7",
   "metadata": {},
   "source": [
    "Split the data frame in a train-test form (70% train - 30% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b58853e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960.4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much examples must be in the 70% training data\n",
    "1372*0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed10569e",
   "metadata": {},
   "source": [
    "This tell us we need take the first 961 rows to train, and the remaining rows for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ee5aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test\n",
    "bank_train = bank_df[:961]\n",
    "bank_test = bank_df[961:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b1b41",
   "metadata": {},
   "source": [
    "Set up the data in the scikit-learn form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35e95aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix X_train with the features of shape (n_samples, n_features)\n",
    "# Do the same for the real label of each sample in Y_train of shape (n_samples,)\n",
    "# Repeat the procces for the test set\n",
    "\n",
    "X_train_bank, Y_train_bank = bank_train.iloc[:,:4].values, bank_train.iloc[:,-1].values\n",
    "X_test_bank, Y_test_bank = bank_test.iloc[:,:4].values, bank_test.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc8e9c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(961,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_train actually have his respective number of examples? Yes\n",
    "Y_train_bank.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0b291",
   "metadata": {},
   "source": [
    "## Occupancy detection data set procesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aac251e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-04 17:51:00</td>\n",
       "      <td>23.18</td>\n",
       "      <td>27.2720</td>\n",
       "      <td>426.0</td>\n",
       "      <td>721.25</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-04 17:51:59</td>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714.00</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  Temperature  Humidity  Light     CO2  HumidityRatio  \\\n",
       "0 2015-02-04 17:51:00        23.18   27.2720  426.0  721.25       0.004793   \n",
       "1 2015-02-04 17:51:59        23.15   27.2675  429.5  714.00       0.004783   \n",
       "\n",
       "   Occupancy  \n",
       "0          1  \n",
       "1          1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, first we take a quick look of the data \n",
    "occupancy_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22f60de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20560, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many examples and features we have in our data set\n",
    "occupancy_df.shape\n",
    "# 20560 examples, 6 features and 1 output column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a66dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classes</th>\n",
       "      <th>count</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15810</td>\n",
       "      <td>77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4750</td>\n",
       "      <td>23%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classes  count proportion\n",
       "0        0  15810        77%\n",
       "1        1   4750        23%"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Study the proportion of classes over the dataset\n",
    "# Group the df for each label and count the number of samples\n",
    "temp_series = occupancy_df.groupby('Occupancy')['Occupancy'].count()\n",
    "temp_df = pd.DataFrame({'classes':temp_series.index, 'count':temp_series.values})\n",
    "temp_df['proportion'] = (temp_df['count']/occupancy_df.shape[0]).apply('{:.0%}'.format)\n",
    "temp_df\n",
    "# The result df give us the respective proportion of each label over the total of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068acc7",
   "metadata": {},
   "source": [
    "We can conclude our data set is mild imbalanced. Although, we'll keep forward to see if models generalize well with the original distribution of the data. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e605c1",
   "metadata": {},
   "source": [
    "Now, we need to analyce each column to see if the features have good behavior or we need to manipulate them in some way. <br> \n",
    "\n",
    "First of all, note that the date column is in a non-processable form for the model. Let's study it a little bit to know what kind of date time information store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea963e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2015], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Year of data\n",
    "occupancy_df.date.dt.year.unique()\n",
    "# Only 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22dffd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Month of data\n",
    "occupancy_df.date.dt.month.unique()\n",
    "# Only february data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "912002de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5,  6,  7,  8,  9, 10,  2,  3, 11, 12, 13, 14, 15, 16, 17, 18],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Days of data\n",
    "occupancy_df.date.dt.day.unique()\n",
    "# Some days of february"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8adf600e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 18, 19, 20, 21, 22, 23,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9,\n",
       "       10, 11, 12, 13, 14, 15, 16], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hours of data\n",
    "occupancy_df.date.dt.hour.unique()\n",
    "# We have records of all hours from the above days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "743cfd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51, 53, 54, 55, 57, 58,  0,  1,  2,  3,  4,  6,  7,  8, 10, 11, 13,\n",
       "       14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 32, 33,\n",
       "       34, 35, 36, 38, 39, 40, 42, 43, 45, 46, 47, 48, 49, 52, 56, 59,  5,\n",
       "        9, 12, 18, 25, 31, 37, 41, 44, 50], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minutes of data\n",
    "occupancy_df.date.dt.minute.unique()\n",
    "# We have records of all minutes for each hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00037e6e",
   "metadata": {},
   "source": [
    "Because we have only data from february 2015, this two date time elements are negligible to our model. Though days vary from 4 up to 18 of february, a more rich information is hours and minutes. Intuitively, the occupancy of a room has a correlation with the hour of the day; must be no the same for day or night. Remember the data was collected of records taken every minute, so, take this feature as the hour and minute of the day is a good handcrafted method. <br>\n",
    " \n",
    "However, we are assuming the description we have of data is true, but to be sure, lets see if the number of examples for every hour is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "698973a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>count</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>915</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>885</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>900</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>915</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>885</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>900</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>915</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>885</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>834</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>776</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>708</td>\n",
       "      <td>3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>720</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>732</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>761</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>840</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>854</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>835</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>900</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>915</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>885</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>900</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>915</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>885</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hour  count proportion\n",
       "0      0    900         4%\n",
       "1      1    915         4%\n",
       "2      2    885         4%\n",
       "3      3    900         4%\n",
       "4      4    915         4%\n",
       "5      5    885         4%\n",
       "6      6    900         4%\n",
       "7      7    915         4%\n",
       "8      8    885         4%\n",
       "9      9    834         4%\n",
       "10    10    776         4%\n",
       "11    11    708         3%\n",
       "12    12    720         4%\n",
       "13    13    732         4%\n",
       "14    14    761         4%\n",
       "15    15    840         4%\n",
       "16    16    854         4%\n",
       "17    17    835         4%\n",
       "18    18    900         4%\n",
       "19    19    915         4%\n",
       "20    20    885         4%\n",
       "21    21    900         4%\n",
       "22    22    915         4%\n",
       "23    23    885         4%"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of the occupancy dataframe to extract only the hour of the day from the date column\n",
    "test_df = occupancy_df.copy()\n",
    "test_df['hour'] = test_df.date.dt.hour\n",
    "\n",
    "# Calculate the proportion of examples for each hour\n",
    "temp_series = test_df.groupby('hour')['hour'].count()\n",
    "temp_df = pd.DataFrame({'hour':temp_series.index, 'count':temp_series.values})\n",
    "temp_df['proportion'] = (temp_df['count']/occupancy_df.shape[0]).apply('{:.0%}'.format)\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56a644",
   "metadata": {},
   "source": [
    "We can conclude the number of examples for each hour is balanced, moreover, the description we have over the data is correct. <br>\n",
    "\n",
    "We decide to keep only with hours and minutes. So, the one date feature will become two date features: hour and minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11321f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns with hour and minute\n",
    "occupancy_df['hour'] = occupancy_df['date'].dt.hour\n",
    "occupancy_df['minute'] = occupancy_df['date'].dt.minute\n",
    "# Drop the date column\n",
    "occupancy_df.drop(columns='date', inplace = True)\n",
    "# Re-arange the columns to keep the labels in last column\n",
    "occupancy_df = occupancy_df[['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio', 'hour', 'minute', 'Occupancy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af78e0f",
   "metadata": {},
   "source": [
    "Since this point, the data process will be the same ast he banknote data set process. We're gonna shuffle tha data and extract train and test sets in 70% - 30% proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d4608fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "occupancy_df = occupancy_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad150bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14391.999999999998"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much examples must be in the train set\n",
    "20560*0.7\n",
    "# Must be 1392 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "145ccccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test\n",
    "occupancy_train = occupancy_df[:14392]\n",
    "occupancy_test = occupancy_df[14392:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "581f673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix X_train with the features of shape (n_samples, n_features)\n",
    "# Do the same for the real label of each sample in Y_train of shape (n_samples,)\n",
    "# Repeat the procces for the test set\n",
    "\n",
    "X_train_occupancy, Y_train_occupancy = occupancy_train.iloc[:,:7].values, occupancy_train.iloc[:,-1].values\n",
    "X_test_occupancy, Y_test_occupancy = occupancy_test.iloc[:,:7].values, occupancy_test.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c0d63ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14392,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y_train actually have his respective number of examples? Yes\n",
    "Y_train_occupancy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b6df1",
   "metadata": {},
   "source": [
    "# Models Implementation\n",
    "\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc854bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_train(X_train, Y_train):\n",
    "    '''\n",
    "    Import and fit a logistic regression model from scikit-learn.\n",
    "    \n",
    "    Input:\n",
    "    X_train: Training X set of samples and features. Shape: (# of samples, # of features)\n",
    "    Y_train: Training Y set of labels for the samples in X_train. Shape: (# of samples, )\n",
    "    \n",
    "    Output: Trained logistic regression model\n",
    "    '''\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression().fit(X_train, Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5051190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic model for both data sets \n",
    "\n",
    "# Banknotes model \n",
    "LogReg_bank = LogisticRegression_train(X_train_bank, Y_train_bank)\n",
    "\n",
    "# Occupancy model\n",
    "LogReg_occupancy = LogisticRegression_train(X_train_occupancy, Y_train_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7261b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predicted labels test and train sets for both data sets, banknotes and occupancy\n",
    "\n",
    "# Banknotes set predicts \n",
    "LogReg_train_bank_predicts = LogReg_bank.predict(X_train_bank)\n",
    "LogReg_test_bank_predicts = LogReg_bank.predict(X_test_bank)\n",
    "\n",
    "# Occupancy set predicts\n",
    "LogReg_train_occupancy_predicts = LogReg_occupancy.predict(X_train_occupancy)\n",
    "LogReg_test_occupancy_predicts = LogReg_occupancy.predict(X_test_occupancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88dfe96",
   "metadata": {},
   "source": [
    "### Performance metrics by confusion matrix example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c924053",
   "metadata": {},
   "source": [
    "Rememeber, here we want to measure the performance of our model based in the precision score (PS) and for standar studies the accuracy score (AS). The following code cells calculate the PS and AS by seen the confusion matrix for our logistic model implemented above and manually calculate the PS and AS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4ce8a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[429,   3],\n",
       "       [  7, 522]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the confusion matrix for sklearn and see it for the training set\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_train_bank, LogReg_train_bank_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69a53971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(429, 3, 7, 522)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also extract the TP, FP, FN, TP number of cases from the matrix \n",
    "tn, fp, fn, tp = confusion_matrix(Y_train_bank, LogReg_train_bank_predicts).ravel()\n",
    "tn, fp, fn, tp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d12ecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For banknote authentication training set, the AS is 0.9895941727367326 and the PS is 0.9942857142857143\n"
     ]
    }
   ],
   "source": [
    "# Calculate the PS and AS for training set\n",
    "PS = tp / (tp + fp)\n",
    "AS = (tp + tn) / (tp + fp + fn + tn)\n",
    "print('For banknote authentication training set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55c108de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[177,   1],\n",
       "       [  3, 230]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the confusion matrix for test set\n",
    "confusion_matrix(Y_test_bank, LogReg_test_bank_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcad137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For banknote authentication test set, the AS is 0.9902676399026764 and the PS is 0.9956709956709957\n"
     ]
    }
   ],
   "source": [
    "# Extact the tp, fp, fn, tp cases and calculate the PS score for test set\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test_bank, LogReg_test_bank_predicts).ravel()\n",
    "PS = tp / (tp + fp)\n",
    "AS = (tp + tn) / (tp + fp + fn + tn)\n",
    "print('For banknote authentication test set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09c9e6",
   "metadata": {},
   "source": [
    "Now we'll see how to do the same process directly without calculate the confusion matrix. <br>\n",
    "\n",
    "### Performance metrics in banknote authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f80415a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For banknote authentication training set, the AS is 0.9895941727367326 and the PS is 0.9942857142857143\n"
     ]
    }
   ],
   "source": [
    "# Import the PS and AS metrics from sklearn and \n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Compute the metrics for train set\n",
    "PS = precision_score(Y_train_bank, LogReg_train_bank_predicts, average='binary')\n",
    "AS = accuracy_score(Y_train_bank, LogReg_train_bank_predicts)\n",
    "print('For banknote authentication training set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd894f",
   "metadata": {},
   "source": [
    "We see both PS and AS are close to one in our training set. This means we have **low bias** in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad6fb631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For banknote authentication test set, the AS is 0.9902676399026764 and the PS is 0.9956709956709957\n"
     ]
    }
   ],
   "source": [
    "# AS and PS for test set\n",
    "PS = precision_score(Y_test_bank, LogReg_test_bank_predicts, average='binary')\n",
    "AS = accuracy_score(Y_test_bank, LogReg_test_bank_predicts)\n",
    "print('For banknote authentication test set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827acff",
   "metadata": {},
   "source": [
    "Both PS and AS in our test set are also close to one. This means we have also **low variance** because training and test errors are close. Moreover, note both metrics are better in our test set than in our training set, this along with low bias and low variance is a good insight of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c696a",
   "metadata": {},
   "source": [
    "### Performance metrics in occupancy problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f474b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For occupancy detection training set, the AS is 0.9895775430794886 and the PS is 0.9954941423851006\n"
     ]
    }
   ],
   "source": [
    "# From sklearn import RS metric and calculate RS and AS for training set\n",
    "from sklearn.metrics import recall_score\n",
    "RS = recall_score(Y_train_occupancy, LogReg_train_occupancy_predicts)\n",
    "AS = accuracy_score(Y_train_occupancy, LogReg_train_occupancy_predicts)\n",
    "print('For occupancy detection training set, the AS is', AS, 'and the PS is', RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb66364",
   "metadata": {},
   "source": [
    "We see **low bias** for both metrics in occupancy detection training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2204a044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For occupancy detection test set, the AS is 0.9871919584954605 and the PS is 0.9922589725545391\n"
     ]
    }
   ],
   "source": [
    "# Calculate RS and AS over the test set\n",
    "RS = recall_score(Y_test_occupancy, LogReg_test_occupancy_predicts)\n",
    "AS = accuracy_score(Y_test_occupancy, LogReg_test_occupancy_predicts)\n",
    "print('For occupancy detection test set, the AS is', AS, 'and the PS is', RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216719c",
   "metadata": {},
   "source": [
    "Also, we see **low variance** when we compare training and test performance metrics (are close for both AS and RS). Again, this can allow us to generalize the models to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f5071",
   "metadata": {},
   "source": [
    "## Decision tree learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24492a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree_train(X_train, Y_train):\n",
    "    '''\n",
    "    Import and fit decision tree learning model from scikit-learn.\n",
    "    \n",
    "    Input:\n",
    "    X_train: Training X set of samples and features. Shape: (# of samples, # of features)\n",
    "    Y_train: Training Y set of labels for the samples in X_train. Shape: (# of samples, )\n",
    "    \n",
    "    Output: Trained decision tree\n",
    "    '''\n",
    "    \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier().fit(X_train, Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2e1a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a decision tree model for both data sets \n",
    "\n",
    "# Banknotes model \n",
    "DecTree_bank = DecisionTree_train(X_train_bank, Y_train_bank)\n",
    "\n",
    "# Occupancy model\n",
    "DecTree_occupancy = DecisionTree_train(X_train_occupancy, Y_train_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11617bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predicted labels test and train sets for both data sets, banknotes and occupancy\n",
    "\n",
    "# Banknotes set predicts \n",
    "DecTree_train_bank_predicts = DecTree_bank.predict(X_train_bank)\n",
    "DecTree_test_bank_predicts = DecTree_bank.predict(X_test_bank)\n",
    "\n",
    "# Occupancy set predicts\n",
    "DecTree_train_occupancy_predicts = DecTree_occupancy.predict(X_train_occupancy)\n",
    "DecTree_test_occupancy_predicts = DecTree_occupancy.predict(X_test_occupancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25319180",
   "metadata": {},
   "source": [
    "### Performance metrics in banknote authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2661a803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For banknote authentication training set, the AS is 1.0 and the PS is 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the metrics for train set\n",
    "PS = precision_score(Y_train_bank, DecTree_train_bank_predicts, average='binary')\n",
    "AS = accuracy_score(Y_train_bank, DecTree_train_bank_predicts)\n",
    "print('For banknote authentication training set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9104a6",
   "metadata": {},
   "source": [
    "Note both performance metrics are close to one. This is, we have **low bias**. We can think perfect behavior (both metrics are one) wouldn't allow model generalize over new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0262f9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For banknote authentication test set, the AS is 0.9805352798053528 and the PS is 0.9828326180257511\n"
     ]
    }
   ],
   "source": [
    "# AS and PS for test set\n",
    "PS = precision_score(Y_test_bank, DecTree_test_bank_predicts, average='binary')\n",
    "AS = accuracy_score(Y_test_bank, DecTree_test_bank_predicts)\n",
    "print('For banknote authentication test set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc9a3a",
   "metadata": {},
   "source": [
    "However, the test performance metrics are close to train metrics. Mereover, both are close to one. This means we have **low variance**. Both, low bias and variance allow us to say we can generalize the model over new banknote data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9fe4c",
   "metadata": {},
   "source": [
    "### Performance metrics in occupancy detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05326d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For occupancy detection training set, the AS is 1.0 and the PS is 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute RS and AS for training set\n",
    "RS = recall_score(Y_train_occupancy, DecTree_train_occupancy_predicts)\n",
    "AS = accuracy_score(Y_train_occupancy, DecTree_train_occupancy_predicts)\n",
    "print('For occupancy detection training set, the AS is', AS, 'and the PS is', RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba8394",
   "metadata": {},
   "source": [
    "Note the **low bias** of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28f275fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For occupancy detection test set, the AS is 0.9920557717250325 and the PS is 0.9845179451090781\n"
     ]
    }
   ],
   "source": [
    "# Calculate RS and AS over the test set\n",
    "RS = recall_score(Y_test_occupancy, DecTree_test_occupancy_predicts)\n",
    "AS = accuracy_score(Y_test_occupancy, DecTree_test_occupancy_predicts)\n",
    "print('For occupancy detection test set, the AS is', AS, 'and the PS is', RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19769343",
   "metadata": {},
   "source": [
    "Both metrics are close to one and even close to them respective metric in the training set, this is **low variance**. Again, we can suppose the model will generalize over new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b832c7f",
   "metadata": {},
   "source": [
    "## Support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219015c4",
   "metadata": {},
   "source": [
    "For this implementation of support vector machines we're gonna do a grid search for the $C$ hyperparameter of soft SVM. $C$ acts as a penalty for missclasified data points for the decision bundary, allowing us make some errors in the training phase.\n",
    "More information of SVM hyperparameters in: [sklearn SVC](https://scikit-learn.org/stable/modules/svm.html#svm-classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad41f2",
   "metadata": {},
   "source": [
    "### Grid Search for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe9347",
   "metadata": {},
   "source": [
    "A grid search consist of train systematically multiple models with all combinations of hyperparameters (for this example only one parameter). At the end, we keep with the hypterparameters that give us the model with better performance. This function is also found in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a8c5a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_GridSearch(hyperparams_dict, score, X_train, Y_train):\n",
    "    '''\n",
    "    Import and fit support vector machine models with grid search from scikit-learn.\n",
    "    \n",
    "    Input:\n",
    "    hyperparams_dict: Dict of hyperparameters for the SVM algorithm; params like C\n",
    "    score: Metric to compare performances of trained SVM models\n",
    "    X_train: Training X set of samples and features. Shape: (# of samples, # of features)\n",
    "    Y_train: Training Y set of labels for the samples in X_train. Shape: (# of samples, )\n",
    "    \n",
    "    Output: Grid search findings for SVMs models trained\n",
    "    '''\n",
    "    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.svm import LinearSVC\n",
    "    \n",
    "    grid_search = GridSearchCV(LinearSVC(), \n",
    "                               hyperparams_dict, \n",
    "                               scoring = score,\n",
    "                               return_train_score=True, \n",
    "                               cv = 5,\n",
    "                               verbose = 1,\n",
    "                               refit = True)\n",
    "    \n",
    "    return grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c29bf878",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\edwar\\anaconda3\\envs\\ML_mathematics\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We declare the hyperparameters dictionary containing all the C-values to explore in grid search\n",
    "hyperparams_dict = {'C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Fit the SVM for banknote authentication data set (remember the metric is precision)\n",
    "score = 'precision'\n",
    "SVM_bank = SVM_GridSearch(hyperparams_dict, score, X_train_bank, Y_train_bank)\n",
    "\n",
    "# Fit the SVM for room occupancy data set (remeber the metric is recall)\n",
    "score = 'recall'\n",
    "SVM_occupancy = SVM_GridSearch(hyperparams_dict, score, X_train_occupancy, Y_train_occupancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "78f17701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predicted labels test and train sets for both data sets, banknotes and occupancy\n",
    "\n",
    "# Banknotes set predicts \n",
    "SVM_train_bank_predicts = SVM_bank.predict(X_train_bank)\n",
    "SVM_test_bank_predicts = SVM_bank.predict(X_test_bank)\n",
    "\n",
    "# Occupancy set predicts\n",
    "SVM_train_occupancy_predicts = SVM_occupancy.predict(X_train_occupancy)\n",
    "SVM_test_occupancy_predicts = SVM_occupancy.predict(X_test_occupancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69263f",
   "metadata": {},
   "source": [
    "### Performance metrics in banknote authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a9acabdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter value was: {'C': 0.1}\n",
      "For banknote authentication training set, the AS is 0.9895941727367326 and the PS is 0.9942857142857143\n"
     ]
    }
   ],
   "source": [
    "# Extract the best parameter values found in grid search\n",
    "best_parameters = SVM_bank.best_params_\n",
    "print('The best parameter value was:', best_parameters)\n",
    "\n",
    "# Compute the metrics for train set\n",
    "PS = precision_score(Y_train_bank, SVM_train_bank_predicts, average='binary')\n",
    "AS = accuracy_score(Y_train_bank, SVM_train_bank_predicts)\n",
    "print('For banknote authentication training set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490ee25",
   "metadata": {},
   "source": [
    "Note both performance metrics are close to one. This is, we have **low bias**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e98b8712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For banknote authentication test set, the AS is 0.9927007299270073 and the PS is 1.0\n"
     ]
    }
   ],
   "source": [
    "# AS and PS for test set\n",
    "PS = precision_score(Y_test_bank, SVM_test_bank_predicts, average='binary')\n",
    "AS = accuracy_score(Y_test_bank, SVM_test_bank_predicts)\n",
    "print('For banknote authentication test set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7fff3",
   "metadata": {},
   "source": [
    "However, the test performance metrics are close to train metrics. Mereover, both are close to zero. This means we have **low variance**. Both, low bias and variance allow us to say we can generalize the model over new banknote data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010615f",
   "metadata": {},
   "source": [
    "### Performance metrics in occupancy detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9bc6613f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter value was: {'C': 0.0001}\n",
      "For occupancy detection training set, the AS is 0.9891606448026682 and the PS is 0.9966957044157405\n"
     ]
    }
   ],
   "source": [
    "# Extract the best parameter values found in grid search\n",
    "best_parameters = SVM_occupancy.best_params_\n",
    "print('The best parameter value was:', best_parameters)\n",
    "\n",
    "# Compute RS and AS for training set\n",
    "RS = recall_score(Y_train_occupancy, SVM_train_occupancy_predicts)\n",
    "AS = accuracy_score(Y_train_occupancy, SVM_train_occupancy_predicts)\n",
    "print('For occupancy detection training set, the AS is', AS, 'and the PS is', RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d4716d",
   "metadata": {},
   "source": [
    "Note the **low bias** of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8e8d482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For occupancy detection test set, the AS is 0.9878404669260701 and the PS is 0.9964813511611541\n"
     ]
    }
   ],
   "source": [
    "# Calculate RS and AS over the test set\n",
    "RS = recall_score(Y_test_occupancy, SVM_test_occupancy_predicts)\n",
    "AS = accuracy_score(Y_test_occupancy, SVM_test_occupancy_predicts)\n",
    "print('For occupancy detection test set, the AS is', AS, 'and the PS is', RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c35b82c",
   "metadata": {},
   "source": [
    "Both metrics are close to one and even close to them respective metric in the training set, this is **low variance**. Again, we can suppose the model will generalize over new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37eb4af",
   "metadata": {},
   "source": [
    "## K - Nearest neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3d47ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_train(X_train, Y_train, n_neighbors=None):\n",
    "    '''\n",
    "    Import and fit KNN learning model from scikit-learn.\n",
    "    \n",
    "    Input:\n",
    "    X_train: Training X set of samples and features. Shape: (# of samples, # of features)\n",
    "    Y_train: Training Y set of labels for the samples in X_train. Shape: (# of samples, )\n",
    "    n_neighbors: Number of neighbors the KNN algorothim use to compute the data label (sklearn default: 5)\n",
    "    \n",
    "    Output: Trained KNN model\n",
    "    '''\n",
    "    \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier(n_neighbors).fit(X_train, Y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d9001edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a decision tree model for both data sets with a K parameter of 2\n",
    "\n",
    "# Banknotes model \n",
    "KNN_bank = KNN_train(X_train_bank, Y_train_bank, n_neighbors = 2)\n",
    "\n",
    "# Occupancy model\n",
    "KNN_occupancy = KNN_train(X_train_occupancy, Y_train_occupancy, n_neighbors = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31fec566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predicted labels test and train sets for both data sets, banknotes and occupancy\n",
    "\n",
    "# Banknotes set predicts \n",
    "KNN_train_bank_predicts = KNN_bank.predict(X_train_bank)\n",
    "KNN_test_bank_predicts = KNN_bank.predict(X_test_bank)\n",
    "\n",
    "# Occupancy set predicts\n",
    "KNN_train_occupancy_predicts = KNN_occupancy.predict(X_train_occupancy)\n",
    "KNN_test_occupancy_predicts = KNN_occupancy.predict(X_test_occupancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271d931",
   "metadata": {},
   "source": [
    "### Performance metrics in banknote authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8cfe78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For banknote authentication training set, the AS is 0.9989594172736732 and the PS is 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the metrics for train set\n",
    "PS = precision_score(Y_train_bank, KNN_train_bank_predicts, average='binary')\n",
    "AS = accuracy_score(Y_train_bank, KNN_train_bank_predicts)\n",
    "print('For banknote authentication training set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b5c05006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For banknote authentication test set, the AS is 1.0 and the PS is 1.0\n"
     ]
    }
   ],
   "source": [
    "# AS and PS for test set\n",
    "PS = precision_score(Y_test_bank, KNN_test_bank_predicts, average='binary')\n",
    "AS = accuracy_score(Y_test_bank, KNN_test_bank_predicts)\n",
    "print('For banknote authentication test set, the AS is', AS, 'and the PS is', PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d044523",
   "metadata": {},
   "source": [
    "We have prefect behavior in training phase, that we can think as overfitting, but when we see test phase assess we see a very high performance too. We have a **low bias** and **low variance**. Therefore we can generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f061a4e",
   "metadata": {},
   "source": [
    "### Performance metrics in occupancy detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eabb927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For occupancy detection training set, the AS is 1.0 and the PS is 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute RS and AS for training set\n",
    "RS = recall_score(Y_train_occupancy, DecTree_train_occupancy_predicts)\n",
    "AS = accuracy_score(Y_train_occupancy, DecTree_train_occupancy_predicts)\n",
    "print('For occupancy detection training set, the AS is', AS, 'and the PS is', RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1fd00c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For occupancy detection test set, the AS is 0.9920557717250325 and the PS is 0.9845179451090781\n"
     ]
    }
   ],
   "source": [
    "# Calculate RS and AS over the test set\n",
    "RS = recall_score(Y_test_occupancy, DecTree_test_occupancy_predicts)\n",
    "AS = accuracy_score(Y_test_occupancy, DecTree_test_occupancy_predicts)\n",
    "print('For occupancy detection test set, the AS is', AS, 'and the PS is', RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d04415",
   "metadata": {},
   "source": [
    "Again, we have **low bias** and **low variance**. We can generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6a4af",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56c1ac",
   "metadata": {},
   "source": [
    "The finding results for our models will be resumed in the following tables, each one for each dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6732b82",
   "metadata": {},
   "source": [
    "**Banknote authentication data set**:\n",
    "\n",
    "| Algorithm | $C$ | Precision in Train | Precision in test | Accuracy in train | Accuracy in test |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Logistic Regression |  | 0.99 | 0.99 | 0.98 | 0.98 |\n",
    "| Decision Tree |  | 1 | 0.98 | 1 | 0.98 |\n",
    "| Support Vector Machine | 0.1 | 0.99 | 1 | 0.98 | 0.99 |\n",
    "| K-Nearest Neighbors |  | 1 | 1 | 0.99 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202d2ad",
   "metadata": {},
   "source": [
    "**Occupancy detection data set**:\n",
    "\n",
    "| Algorithm | $C$ | Precision in Train | Precision in test | Accuracy in train | Accuracy in test |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Logistic Regression |  | 0.99 | 0.99 | 0.98 | 0.98 |\n",
    "| Decision Tree |  | 1 | 0.98 | 1 | 0.99 |\n",
    "| Support Vector Machine | 0.0001 | 0.99 | 0.99 | 0.98 | 0.98 |\n",
    "| K-Nearest Neighbors |  | 1 | 0.98 | 1 | 0.99 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255988d",
   "metadata": {},
   "source": [
    "# Conclusions, Discussions and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb8275",
   "metadata": {},
   "source": [
    "We have some general conclusions:\n",
    "<ol>\n",
    "  <li>All the four models can generalize to new data.</li>\n",
    "  <li>All the four models have almost perfect performance (some of them a perfect performance) in both, training and test sets.</li>\n",
    "  <li>These data sets are good for learning to implement models, not for practice feature engineering (the data already have a ready to pass form).</li>\n",
    "</ol>\n",
    "\n",
    "Note: The possible results in new data of the chosen model depend on how much the new data resembles the training and test data. For these two particular datasets, we have the data in a highly processed form. For banknote authentication, computing the wavelet transform of banknote pictures may not be as fast as we think. Similarly, extracting all the features of a room to detect occupancy in real time can involve expensive infrastructure costs.\n",
    "\n",
    "Real-world scenarios involve highly complex datasets. Preprocessing and performing valuable feature engineering are probably the most important and time-consuming aspects in an ML project. Future work can aim to explore more complex open datasets and iterate over a data methodology, such as CRIPS DM, to develop valuable business insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268.225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
